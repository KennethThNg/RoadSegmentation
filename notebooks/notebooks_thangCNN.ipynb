{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Road segmentation according to kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we try to implement the CNN for the road segmentation for the ML course project, a files is provided, but here, we try another approach form an another source."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Package importation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "import os,sys\n",
    "from PIL import Image\n",
    "import tensorflow.python.platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import time\n",
    "from datetime import timedelta\n",
    "import math\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import math\n",
    "import operator\n",
    "import functools\n",
    "import random\n",
    "import datetime\n",
    "from scipy.ndimage.interpolation import rotate, shift, zoom\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CHANNELS = 3 # RGB images\n",
    "PIXEL_DEPTH = 255\n",
    "NUM_LABELS = 2\n",
    "TRAINING_SIZE = 20\n",
    "VALIDATION_SIZE = 5  # Size of the validation set.\n",
    "SEED = 66478  # Set to None for random seed.\n",
    "BATCH_SIZE = 16 # 64\n",
    "NUM_EPOCHS = TRAINING_ITERS\n",
    "RESTORE_MODEL = False # If True, restore existing model instead of training a new one\n",
    "RECORDING_STEP = 20\n",
    "IMG_PATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = '../data/'\n",
    "data_dir = data_folder + 'training/'\n",
    "train_data_filename = data_dir + 'images/' #input X\n",
    "train_labels_filename = data_dir + 'groundtruth/' #output Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract patches from a given image\n",
    "def img_crop(im, w, h):\n",
    "    list_patches = []\n",
    "    imgwidth = im.shape[0]\n",
    "    imgheight = im.shape[1]\n",
    "    is_2d = len(im.shape) < 3\n",
    "    for i in range(0,imgheight,h):\n",
    "        for j in range(0,imgwidth,w):\n",
    "            if is_2d:\n",
    "                im_patch = im[j:j+w, i:i+h]\n",
    "            else:\n",
    "                im_patch = im[j:j+w, i:i+h, :]\n",
    "            list_patches.append(im_patch)\n",
    "    return list_patches\n",
    "\n",
    "def extract_data(filename, num_images):\n",
    "    \"\"\"Extract the images into a 4D tensor [image index, y, x, channels].\n",
    "    Values are rescaled from [0, 255] down to [-0.5, 0.5].\n",
    "    \"\"\"\n",
    "    imgs = []\n",
    "    for i in range(1, num_images+1):\n",
    "        imageid = \"satImage_%.3d\" % i\n",
    "        image_filename = filename + imageid + \".png\"\n",
    "        if os.path.isfile(image_filename):\n",
    "            print ('Loading ' + image_filename)\n",
    "            img = mpimg.imread(image_filename)\n",
    "            imgs.append(img)\n",
    "        else:\n",
    "            print ('File ' + image_filename + ' does not exist')\n",
    "\n",
    "    num_images = len(imgs)\n",
    "    IMG_WIDTH = imgs[0].shape[0]\n",
    "    IMG_HEIGHT = imgs[0].shape[1]\n",
    "    N_PATCHES_PER_IMAGE = (IMG_WIDTH/IMG_PATCH_SIZE)*(IMG_HEIGHT/IMG_PATCH_SIZE)\n",
    "\n",
    "    img_patches = [img_crop(imgs[i], IMG_PATCH_SIZE, IMG_PATCH_SIZE) for i in range(num_images)]\n",
    "    data = [img_patches[i][j] for i in range(len(img_patches)) for j in range(len(img_patches[i]))]\n",
    "\n",
    "    return np.asarray(data)\n",
    "        \n",
    "# Assign a label to a patch v\n",
    "def value_to_class(v):\n",
    "    foreground_threshold = 0.25 # percentage of pixels > 1 required to assign a foreground label to a patch\n",
    "    df = np.sum(v)\n",
    "    if df > foreground_threshold:\n",
    "        return [0, 1]\n",
    "    else:\n",
    "        return [1, 0]\n",
    "\n",
    "# Extract label images\n",
    "def extract_labels(filename, num_images):\n",
    "    \"\"\"Extract the labels into a 1-hot matrix [image index, label index].\"\"\"\n",
    "    gt_imgs = []\n",
    "    for i in range(1, num_images+1):\n",
    "        imageid = \"satImage_%.3d\" % i\n",
    "        image_filename = filename + imageid + \".png\"\n",
    "        if os.path.isfile(image_filename):\n",
    "            print ('Loading ' + image_filename)\n",
    "            img = mpimg.imread(image_filename)\n",
    "            gt_imgs.append(img)\n",
    "        else:\n",
    "            print ('File ' + image_filename + ' does not exist')\n",
    "\n",
    "    num_images = len(gt_imgs)\n",
    "    gt_patches = [img_crop(gt_imgs[i], IMG_PATCH_SIZE, IMG_PATCH_SIZE) for i in range(num_images)]\n",
    "    data = np.asarray([gt_patches[i][j] for i in range(len(gt_patches)) for j in range(len(gt_patches[i]))])\n",
    "    labels = np.asarray([value_to_class(np.mean(data[i])) for i in range(len(data))])\n",
    "\n",
    "    # Convert to dense 1-hot representation.\n",
    "    return labels.astype(np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data/training/images/satImage_001.png\n",
      "Loading data/training/images/satImage_002.png\n",
      "Loading data/training/images/satImage_003.png\n",
      "Loading data/training/images/satImage_004.png\n",
      "Loading data/training/images/satImage_005.png\n",
      "Loading data/training/images/satImage_006.png\n",
      "Loading data/training/images/satImage_007.png\n",
      "Loading data/training/images/satImage_008.png\n",
      "Loading data/training/images/satImage_009.png\n",
      "Loading data/training/images/satImage_010.png\n",
      "Loading data/training/images/satImage_011.png\n",
      "Loading data/training/images/satImage_012.png\n",
      "Loading data/training/images/satImage_013.png\n",
      "Loading data/training/images/satImage_014.png\n",
      "Loading data/training/images/satImage_015.png\n",
      "Loading data/training/images/satImage_016.png\n",
      "Loading data/training/images/satImage_017.png\n",
      "Loading data/training/images/satImage_018.png\n",
      "Loading data/training/images/satImage_019.png\n",
      "Loading data/training/images/satImage_020.png\n",
      "Loading data/training/groundtruth/satImage_001.png\n",
      "Loading data/training/groundtruth/satImage_002.png\n",
      "Loading data/training/groundtruth/satImage_003.png\n",
      "Loading data/training/groundtruth/satImage_004.png\n",
      "Loading data/training/groundtruth/satImage_005.png\n",
      "Loading data/training/groundtruth/satImage_006.png\n",
      "Loading data/training/groundtruth/satImage_007.png\n",
      "Loading data/training/groundtruth/satImage_008.png\n",
      "Loading data/training/groundtruth/satImage_009.png\n",
      "Loading data/training/groundtruth/satImage_010.png\n",
      "Loading data/training/groundtruth/satImage_011.png\n",
      "Loading data/training/groundtruth/satImage_012.png\n",
      "Loading data/training/groundtruth/satImage_013.png\n",
      "Loading data/training/groundtruth/satImage_014.png\n",
      "Loading data/training/groundtruth/satImage_015.png\n",
      "Loading data/training/groundtruth/satImage_016.png\n",
      "Loading data/training/groundtruth/satImage_017.png\n",
      "Loading data/training/groundtruth/satImage_018.png\n",
      "Loading data/training/groundtruth/satImage_019.png\n",
      "Loading data/training/groundtruth/satImage_020.png\n"
     ]
    }
   ],
   "source": [
    "train_data = extract_data(train_data_filename, TRAINING_SIZE) #Input, real images\n",
    "train_labels = extract_labels(train_labels_filename, TRAINING_SIZE) #Output, black and white image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be consistent and not always change the variable name, we will use the same denomination as [Kaggle](https://www.kaggle.com/pouryaayria/convolutional-neural-networks-tutorial-tensorflow) . But we don't need to split the data since they seems already to be split into a train set and the dataset is really small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (images) shape: (12500, 16, 16, 3)\n",
      "Training set (labels) shape: (12500, 2)\n"
     ]
    }
   ],
   "source": [
    "# Shapes of training set\n",
    "print(\"Training set (images) shape: {shape}\".format(shape=train_data.shape))\n",
    "print(\"Training set (labels) shape: {shape}\".format(shape=train_labels.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The datasets contains a sample of 12500 images, which seems not to be large, then a splitting won't be necessary. The dataset is a list of matrix of size 16x16 representing the pixels we have chosen and 3 represents the number of colors RGB (number of channels)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration of Neural Network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some important variables have already been defined in the section parameters, we put some here and add other relevant parameters. We add only the parameters ``LEARNING_RATE`` which can be useful later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_SIZE = 20\n",
    "NUM_LABELS = 2\n",
    "BATCH_SIZE = 16 # 64\n",
    "NUM_EPOCHS = 10\n",
    "LEARNING_RATE = 0.001\n",
    "TRAINING_ITERS = 40000\n",
    "DROPOUT = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We try to balance our dataset so that the learning phase is more easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data points per class: c0 = 9450 c1 = 3050\n",
      "Balancing training data...\n",
      "6100\n",
      "(12500, 16, 16, 3)\n"
     ]
    }
   ],
   "source": [
    "c0 = 0\n",
    "c1 = 0\n",
    "for i in range(len(train_labels)):\n",
    "        if train_labels[i][0] == 1:\n",
    "            c0 = c0 + 1\n",
    "        else:\n",
    "            c1 = c1 + 1\n",
    "print ('Number of data points per class: c0 = ' + str(c0) + ' c1 = ' + str(c1))\n",
    "\n",
    "print ('Balancing training data...')\n",
    "min_c = min(c0, c1)\n",
    "idx0 = [i for i, j in enumerate(train_labels) if j[0] == 1]\n",
    "idx1 = [i for i, j in enumerate(train_labels) if j[1] == 1]\n",
    "new_indices = idx0[0:min_c] + idx1[0:min_c]\n",
    "print (len(new_indices))\n",
    "print (train_data.shape)\n",
    "train_data = train_data[new_indices,:,:,:]\n",
    "train_labels = train_labels[new_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data points per class: c0 = 3050 c1 = 3050\n"
     ]
    }
   ],
   "source": [
    "train_size = train_labels.shape[0]\n",
    "\n",
    "c0 = 0\n",
    "c1 = 0\n",
    "for i in range(len(train_labels)):\n",
    "    if train_labels[i][0] == 1:\n",
    "        c0 = c0 + 1\n",
    "    else:\n",
    "        c1 = c1 + 1\n",
    "print ('Number of data points per class: c0 = ' + str(c0) + ' c1 = ' + str(c1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Same padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of placeholder (16, 16, 16, 3) (16, 2)\n"
     ]
    }
   ],
   "source": [
    "train_data_node = tf.placeholder(\n",
    "    tf.float32,\n",
    "    shape=(BATCH_SIZE, IMG_PATCH_SIZE, IMG_PATCH_SIZE, NUM_CHANNELS))\n",
    "train_labels_node = tf.placeholder(tf.float32,\n",
    "                                       shape=(BATCH_SIZE, NUM_LABELS))\n",
    "train_all_data_node = tf.constant(train_data)\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "print('Shape of placeholder',train_data_node.shape, train_labels_node.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d(x, W, b, strides=1):\n",
    "    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')\n",
    "    x = tf.nn.bias_add(x, b)\n",
    "    return tf.nn.relu(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Max pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maxpool2d(x, k=2):\n",
    "    return tf.nn.max_pool(x,\n",
    "                              ksize=[1, k, k, 1],\n",
    "                              strides=[1, k, k, 1],\n",
    "                              padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = {\n",
    "    'wc1': tf.Variable(tf.truncated_normal([5, 5, NUM_CHANNELS, 32],  # 5x5 filter, depth 32.\n",
    "                                stddev=0.1,\n",
    "                                seed=SEED)),\n",
    "    'wc2': tf.Variable(tf.truncated_normal([5, 5, 32, 64],\n",
    "                                stddev=0.1,\n",
    "                                seed=SEED)),\n",
    "    'wd1': tf.Variable(  # fully connected, depth 512.\n",
    "    tf.truncated_normal([int(IMG_PATCH_SIZE / 4 * IMG_PATCH_SIZE / 4 * 64), 512],\n",
    "                                stddev=0.1,\n",
    "                                seed=SEED)),\n",
    "    'out': tf.Variable(\n",
    "    tf.truncated_normal([512, NUM_LABELS],\n",
    "                                stddev=0.1,\n",
    "                                seed=SEED)),\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'bc1':  tf.Variable(tf.zeros([32])),\n",
    "    'bc2':  tf.Variable(tf.constant(0.1, shape=[64])),\n",
    "    'bd1':  tf.Variable(tf.constant(0.1, shape=[512])),\n",
    "    'out':  tf.Variable(tf.constant(0.1, shape=[NUM_LABELS]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_net(x, weights, biases, dropout):\n",
    "    #Convolution layer 1\n",
    "    conv1 = conv2d(x, weights['wc1'], biases['bc1'])\n",
    "    # Max pooling\n",
    "    conv1 = maxpool2d(conv1, k=2)\n",
    "    \n",
    "    #Convolution layer 2\n",
    "    conv2 = conv2d(conv1, weights['wc2'], biases['bc2'])\n",
    "    # Max pooling\n",
    "    conv2 = maxpool2d(conv2, k=2)\n",
    "    \n",
    "    # Reshape the feature map cuboid into a 2D matrix to feed it to the\n",
    "    # fully connected layers.\n",
    "    conv2_shape = conv2.get_shape().as_list()\n",
    "    fc1 = tf.reshape(conv2, shape = [conv2_shape[0], conv2_shape[1] * conv2_shape[2] * conv2_shape[3]])\n",
    "    fc1 = tf.add(tf.matmul(fc1,weights['wd1']), biases['bd1'])\n",
    "    fc1 = tf.nn.relu(fc1)#layer\n",
    "    fc1 = tf.nn.dropout(fc1, dropout)\n",
    "    \n",
    "    out = tf.add(tf.matmul(fc1,weights['out']), biases['out'])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a cost function to minimize. In [Kaggle](https://www.kaggle.com/pouryaayria/convolutional-neural-networks-tutorial-tensorflow), they propose to use AdamOptimizer which is an advanced form of Gradient Descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = train_data_node\n",
    "y = train_labels_node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Add_5:0\", shape=(16, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Create the model\n",
    "model = conv_net(x, weights, biases, keep_prob)\n",
    "print(model)\n",
    "#Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=model, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=LEARNING_RATE).minimize(cost)\n",
    "\n",
    "# Evaluate model\n",
    "y_true_cls = tf.argmax(y,1)\n",
    "y_pred_cls = tf.argmax(model,1)\n",
    "# This is a vector of booleans whether the predicted \n",
    "#class equals the true class of each image.\n",
    "correct_model = tf.equal(y_pred_cls, y_true_cls)\n",
    "# This calculates the classification accuracy by first type-casting \n",
    "#the vector of booleans to floats, so that False becomes 0 and True becomes 1,\n",
    "#and then calculating the average of these numbers.\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_model, tf.float32))\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the number of samples that goings to be propagated through the network, the batch size is the number of training examples present in a single batch. We can't pass the entire dataset into neural net at once, then we divide the dataset into number of batches or sets or parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBatch(X, Y, batchSize=16):\n",
    "    \"\"\"\n",
    "    Creates a list of random minibatches from (X, Y)\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input train/test \n",
    "    Y --input label train/test\n",
    "    \n",
    "    Returns:\n",
    "    mini_batches -- tuple of synchronous (mini_batch_X, mini_batch_Y)\n",
    "    \n",
    "    \"\"\"\n",
    "    arrayLength = X.shape[0]\n",
    "    count = 0 \n",
    "    \n",
    "    while count < arrayLength/batchSize:\n",
    "        random.seed(datetime.datetime.now())\n",
    "        randstart = random.randint(0, arrayLength-batchSize-1)\n",
    "#         print(randstart)\n",
    "        count += 1\n",
    "        yield (X[randstart:randstart+batchSize], Y[randstart:randstart+batchSize]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************\n",
      "Epoch 320, Loss= 0.493, Training Accuracy= 0.750\n",
      "***************\n",
      "Epoch 640, Loss= 1.399, Training Accuracy= 0.000\n",
      "***************\n",
      "Epoch 960, Loss= 0.176, Training Accuracy= 1.000\n",
      "***************\n",
      "Epoch 1280, Loss= 1.108, Training Accuracy= 0.000\n",
      "***************\n",
      "Epoch 1600, Loss= 0.552, Training Accuracy= 0.750\n",
      "***************\n",
      "Epoch 1920, Loss= 0.503, Training Accuracy= 0.875\n",
      "***************\n",
      "Epoch 2240, Loss= 0.922, Training Accuracy= 0.125\n",
      "***************\n",
      "Epoch 2560, Loss= 0.423, Training Accuracy= 0.938\n",
      "***************\n",
      "Epoch 2880, Loss= 0.484, Training Accuracy= 0.875\n",
      "***************\n",
      "Epoch 3200, Loss= 0.898, Training Accuracy= 0.312\n",
      "***************\n",
      "Epoch 3520, Loss= 0.444, Training Accuracy= 1.000\n",
      "***************\n",
      "Epoch 3840, Loss= 0.586, Training Accuracy= 0.625\n",
      "***************\n",
      "Epoch 4160, Loss= 0.328, Training Accuracy= 1.000\n",
      "***************\n",
      "Epoch 4480, Loss= 0.812, Training Accuracy= 0.125\n",
      "***************\n",
      "Epoch 4800, Loss= 0.601, Training Accuracy= 0.562\n",
      "***************\n",
      "Epoch 5120, Loss= 0.884, Training Accuracy= 0.062\n",
      "***************\n",
      "Epoch 5440, Loss= 0.453, Training Accuracy= 1.000\n",
      "***************\n",
      "Epoch 5760, Loss= 0.603, Training Accuracy= 0.562\n",
      "***************\n",
      "Epoch 6080, Loss= 0.455, Training Accuracy= 0.812\n",
      "***************\n",
      "Epoch 6400, Loss= 0.551, Training Accuracy= 0.875\n",
      "***************\n",
      "Epoch 6720, Loss= 0.464, Training Accuracy= 1.000\n",
      "***************\n",
      "Epoch 7040, Loss= 0.926, Training Accuracy= 0.125\n",
      "***************\n",
      "Epoch 7360, Loss= 0.492, Training Accuracy= 1.000\n",
      "***************\n",
      "Epoch 7680, Loss= 0.385, Training Accuracy= 1.000\n",
      "***************\n",
      "Epoch 8000, Loss= 0.257, Training Accuracy= 1.000\n",
      "***************\n",
      "Epoch 8320, Loss= 0.458, Training Accuracy= 0.688\n",
      "***************\n",
      "Epoch 8640, Loss= 0.245, Training Accuracy= 1.000\n",
      "***************\n",
      "Epoch 8960, Loss= 0.732, Training Accuracy= 0.750\n",
      "***************\n",
      "Epoch 9280, Loss= 0.364, Training Accuracy= 1.000\n",
      "***************\n",
      "Epoch 9600, Loss= 0.459, Training Accuracy= 0.938\n",
      "***************\n",
      "Epoch 9920, Loss= 0.470, Training Accuracy= 0.938\n",
      "***************\n",
      "Epoch 10240, Loss= 0.534, Training Accuracy= 0.938\n",
      "***************\n",
      "Epoch 10560, Loss= 0.340, Training Accuracy= 1.000\n",
      "***************\n",
      "Epoch 10880, Loss= 0.547, Training Accuracy= 0.625\n",
      "***************\n",
      "Epoch 11200, Loss= 0.256, Training Accuracy= 1.000\n",
      "***************\n",
      "Epoch 11520, Loss= 0.564, Training Accuracy= 0.938\n",
      "***************\n",
      "Epoch 11840, Loss= 0.340, Training Accuracy= 1.000\n",
      "***************\n",
      "Epoch 12160, Loss= 0.267, Training Accuracy= 1.000\n",
      "***************\n",
      "Epoch 12480, Loss= 0.415, Training Accuracy= 0.875\n",
      "***************\n",
      "Epoch 12800, Loss= 0.436, Training Accuracy= 0.938\n",
      "***************\n",
      "Epoch 13120, Loss= 0.214, Training Accuracy= 1.000\n",
      "***************\n",
      "Epoch 13440, Loss= 0.585, Training Accuracy= 0.812\n",
      "***************\n",
      "Epoch 13760, Loss= 0.302, Training Accuracy= 0.938\n",
      "***************\n",
      "Epoch 14080, Loss= 0.369, Training Accuracy= 1.000\n",
      "***************\n",
      "Epoch 14400, Loss= 0.448, Training Accuracy= 0.938\n",
      "***************\n",
      "Epoch 14720, Loss= 0.219, Training Accuracy= 1.000\n",
      "***************\n",
      "Epoch 15040, Loss= 0.808, Training Accuracy= 0.188\n",
      "***************\n",
      "Epoch 15360, Loss= 0.854, Training Accuracy= 0.312\n",
      "***************\n",
      "Epoch 15680, Loss= 0.456, Training Accuracy= 0.938\n",
      "***************\n",
      "Epoch 16000, Loss= 0.976, Training Accuracy= 0.062\n",
      "***************\n",
      "Epoch 16320, Loss= 0.591, Training Accuracy= 0.812\n",
      "***************\n",
      "Epoch 16640, Loss= 0.451, Training Accuracy= 1.000\n",
      "***************\n",
      "Epoch 16960, Loss= 0.431, Training Accuracy= 1.000\n",
      "***************\n",
      "Epoch 17280, Loss= 0.478, Training Accuracy= 0.688\n",
      "***************\n",
      "Epoch 17600, Loss= 0.507, Training Accuracy= 0.688\n",
      "***************\n",
      "Epoch 17920, Loss= 0.509, Training Accuracy= 0.812\n",
      "***************\n",
      "Epoch 18240, Loss= 0.435, Training Accuracy= 0.938\n",
      "***************\n",
      "Epoch 18560, Loss= 0.278, Training Accuracy= 1.000\n",
      "***************\n",
      "Epoch 18880, Loss= 1.014, Training Accuracy= 0.062\n",
      "***************\n",
      "Epoch 19200, Loss= 0.717, Training Accuracy= 0.625\n",
      "***************\n",
      "Epoch 19520, Loss= 0.542, Training Accuracy= 0.812\n",
      "***************\n",
      "Epoch 19840, Loss= 0.609, Training Accuracy= 0.500\n",
      "***************\n",
      "Epoch 20160, Loss= 0.515, Training Accuracy= 0.688\n",
      "***************\n",
      "Epoch 20480, Loss= 1.031, Training Accuracy= 0.188\n",
      "***************\n",
      "Epoch 20800, Loss= 0.347, Training Accuracy= 0.875\n",
      "***************\n",
      "Epoch 21120, Loss= 0.399, Training Accuracy= 0.938\n",
      "***************\n",
      "Epoch 21440, Loss= 0.691, Training Accuracy= 0.750\n",
      "***************\n",
      "Epoch 21760, Loss= 0.381, Training Accuracy= 0.750\n",
      "***************\n",
      "Epoch 22080, Loss= 0.884, Training Accuracy= 0.438\n",
      "***************\n",
      "Epoch 22400, Loss= 0.380, Training Accuracy= 0.812\n",
      "***************\n",
      "Epoch 22720, Loss= 1.206, Training Accuracy= 0.188\n",
      "***************\n",
      "Epoch 23040, Loss= 0.373, Training Accuracy= 1.000\n",
      "***************\n",
      "Epoch 23360, Loss= 0.712, Training Accuracy= 0.438\n",
      "***************\n",
      "Epoch 23680, Loss= 0.578, Training Accuracy= 0.938\n",
      "***************\n",
      "Epoch 24000, Loss= 0.466, Training Accuracy= 0.750\n",
      "***************\n",
      "Epoch 24320, Loss= 0.562, Training Accuracy= 0.812\n",
      "***************\n",
      "Epoch 24640, Loss= 0.309, Training Accuracy= 1.000\n",
      "***************\n",
      "Epoch 24960, Loss= 0.371, Training Accuracy= 0.875\n",
      "***************\n",
      "Epoch 25280, Loss= 0.425, Training Accuracy= 0.688\n",
      "***************\n",
      "Epoch 25600, Loss= 0.625, Training Accuracy= 0.688\n",
      "***************\n",
      "Epoch 25920, Loss= 0.736, Training Accuracy= 0.438\n",
      "***************\n",
      "Epoch 26240, Loss= 0.685, Training Accuracy= 0.688\n",
      "***************\n",
      "Epoch 26560, Loss= 0.559, Training Accuracy= 0.875\n",
      "***************\n",
      "Epoch 26880, Loss= 0.956, Training Accuracy= 0.312\n",
      "***************\n",
      "Epoch 27200, Loss= 0.396, Training Accuracy= 0.938\n",
      "***************\n",
      "Epoch 27520, Loss= 1.233, Training Accuracy= 0.000\n",
      "***************\n",
      "Epoch 27840, Loss= 0.585, Training Accuracy= 0.812\n",
      "***************\n",
      "Epoch 28160, Loss= 0.550, Training Accuracy= 0.500\n",
      "***************\n",
      "Epoch 28480, Loss= 0.477, Training Accuracy= 0.875\n",
      "***************\n",
      "Epoch 28800, Loss= 0.307, Training Accuracy= 1.000\n",
      "***************\n",
      "Epoch 29120, Loss= 0.476, Training Accuracy= 0.875\n",
      "***************\n",
      "Epoch 29440, Loss= 0.329, Training Accuracy= 1.000\n",
      "***************\n",
      "Epoch 29760, Loss= 0.575, Training Accuracy= 0.688\n",
      "***************\n",
      "Epoch 30080, Loss= 0.905, Training Accuracy= 0.312\n",
      "***************\n",
      "Epoch 30400, Loss= 0.551, Training Accuracy= 0.812\n",
      "***************\n",
      "Epoch 30720, Loss= 0.510, Training Accuracy= 0.938\n",
      "***************\n",
      "Epoch 31040, Loss= 0.573, Training Accuracy= 0.875\n",
      "***************\n",
      "Epoch 31360, Loss= 0.459, Training Accuracy= 0.875\n",
      "***************\n",
      "Epoch 31680, Loss= 0.676, Training Accuracy= 0.750\n",
      "***************\n",
      "Epoch 32000, Loss= 0.877, Training Accuracy= 0.250\n",
      "***************\n",
      "Epoch 32320, Loss= 0.459, Training Accuracy= 0.812\n",
      "***************\n",
      "Epoch 32640, Loss= 0.429, Training Accuracy= 1.000\n",
      "***************\n",
      "Epoch 32960, Loss= 0.574, Training Accuracy= 0.938\n",
      "***************\n",
      "Epoch 33280, Loss= 0.891, Training Accuracy= 0.250\n",
      "***************\n",
      "Epoch 33600, Loss= 0.862, Training Accuracy= 0.375\n",
      "***************\n",
      "Epoch 33920, Loss= 0.439, Training Accuracy= 0.688\n",
      "***************\n",
      "Epoch 34240, Loss= 0.454, Training Accuracy= 0.625\n",
      "***************\n",
      "Epoch 34560, Loss= 0.259, Training Accuracy= 0.938\n",
      "***************\n",
      "Epoch 34880, Loss= 0.883, Training Accuracy= 0.375\n",
      "***************\n",
      "Epoch 35200, Loss= 0.944, Training Accuracy= 0.250\n",
      "***************\n",
      "Epoch 35520, Loss= 0.509, Training Accuracy= 0.625\n",
      "***************\n",
      "Epoch 35840, Loss= 0.480, Training Accuracy= 1.000\n",
      "***************\n",
      "Epoch 36160, Loss= 0.458, Training Accuracy= 0.938\n",
      "***************\n",
      "Epoch 36480, Loss= 0.704, Training Accuracy= 0.562\n",
      "***************\n",
      "Epoch 36800, Loss= 0.552, Training Accuracy= 0.625\n",
      "***************\n",
      "Epoch 37120, Loss= 0.441, Training Accuracy= 0.625\n",
      "***************\n",
      "Epoch 37440, Loss= 0.376, Training Accuracy= 1.000\n",
      "***************\n",
      "Epoch 37760, Loss= 0.390, Training Accuracy= 1.000\n",
      "***************\n",
      "Epoch 38080, Loss= 0.532, Training Accuracy= 0.938\n",
      "***************\n",
      "Epoch 38400, Loss= 0.563, Training Accuracy= 0.688\n",
      "***************\n",
      "Epoch 38720, Loss= 0.555, Training Accuracy= 0.500\n",
      "***************\n",
      "Epoch 39040, Loss= 0.478, Training Accuracy= 0.875\n",
      "***************\n",
      "Epoch 39360, Loss= 0.334, Training Accuracy= 1.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************\n",
      "Epoch 39680, Loss= 0.462, Training Accuracy= 1.000\n"
     ]
    }
   ],
   "source": [
    "loss_t = []\n",
    "steps_t = []\n",
    "acc_t = []\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    step = 1   \n",
    "#     Keep training until reach max iterations\n",
    "    while step * BATCH_SIZE < NUM_EPOCHS:\n",
    "        a = getBatch(train_data, train_labels, BATCH_SIZE)\n",
    "        batch_x, batch_y = next(a)\n",
    "        sess.run(optimizer, feed_dict={x: batch_x, y: batch_y, keep_prob: DROPOUT})\n",
    "        if step % RECORDING_STEP == 0:\n",
    "            print('*'*15)\n",
    "            loss, acc = sess.run([cost, accuracy], feed_dict={x: batch_x,\n",
    "                                                              y: batch_y,\n",
    "                                                              keep_prob: 1.})\n",
    "            print(\"Epoch \" + str(step*BATCH_SIZE) + \", Loss= \" + \\\n",
    "                  \"{:.3f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                  \"{:.3f}\".format(acc))\n",
    "            loss_t.append(loss)\n",
    "            steps_t.append(step*BATCH_SIZE)\n",
    "            acc_t.append(acc)\n",
    "        step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
