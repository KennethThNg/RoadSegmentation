{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Baseline for machine learning project on road segmentation.\n",
    "This simple baseline consits of a CNN with two convolutional+pooling layers with a soft-max loss\n",
    "Credits: Aurelien Lucchi, ETH ZÃ¼rich\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "import gzip\n",
    "import os\n",
    "import sys\n",
    "import urllib\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "import code\n",
    "\n",
    "import tensorflow.python.platform\n",
    "\n",
    "import numpy \n",
    "import tensorflow as tf\n",
    "\n",
    "NUM_CHANNELS = 3 # RGB images\n",
    "PIXEL_DEPTH = 255\n",
    "NUM_LABELS = 2\n",
    "TRAINING_SIZE = 100\n",
    "VALIDATION_SIZE = 5  # Size of the validation set.\n",
    "SEED = 66478  # Set to None for random seed.\n",
    "BATCH_SIZE = 16 # 64\n",
    "NUM_EPOCHS = 5\n",
    "RESTORE_MODEL = False # If True, restore existing model instead of training a new one\n",
    "RECORDING_STEP = 1000\n",
    "\n",
    "# Set image patch size in pixels\n",
    "# IMG_PATCH_SIZE should be a multiple of 4\n",
    "# image size should be an integer multiple of this number!\n",
    "IMG_PATCH_SIZE = 16\n",
    "\n",
    "#tf.app.flags.DEFINE_string('train_dir', '/tmp/mnist',\n",
    "#                           \"\"\"Directory where to write event logs \"\"\"\n",
    "#                           \"\"\"and checkpoint.\"\"\")\n",
    "#FLAGS = tf.app.flags.FLAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract patches from a given image\n",
    "def img_crop(im, w, h):\n",
    "    list_patches = []\n",
    "    imgwidth = im.shape[0]\n",
    "    imgheight = im.shape[1]\n",
    "    is_2d = len(im.shape) < 3\n",
    "    for i in range(0,imgheight,h):\n",
    "        for j in range(0,imgwidth,w):\n",
    "            if is_2d:\n",
    "                im_patch = im[j:j+w, i:i+h]\n",
    "            else:\n",
    "                im_patch = im[j:j+w, i:i+h, :]\n",
    "            list_patches.append(im_patch)\n",
    "    return list_patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(filename, num_images):\n",
    "    \"\"\"Extract the images into a 4D tensor [image index, y, x, channels].\n",
    "    Values are scaled from [0, 1] instead of [0,255].\n",
    "    \"\"\"\n",
    "    imgs = []\n",
    "    for i in range(1, num_images+1):\n",
    "        imageid = \"satImage_%.3d\" % i\n",
    "        image_filename = filename + imageid + \".png\"\n",
    "        if os.path.isfile(image_filename):\n",
    "            print ('Loading ' + image_filename)\n",
    "            img = mpimg.imread(image_filename)\n",
    "            imgs.append(img)\n",
    "        else:\n",
    "            print ('File ' + image_filename + ' does not exist')\n",
    "\n",
    "    num_images = len(imgs)\n",
    "    IMG_WIDTH = imgs[0].shape[0]\n",
    "    IMG_HEIGHT = imgs[0].shape[1]\n",
    "\n",
    "    img_patches = [img_crop(imgs[i], IMG_PATCH_SIZE, IMG_PATCH_SIZE) for i in range(num_images)]\n",
    "    data = [img_patches[i][j] for i in range(len(img_patches)) for j in range(len(img_patches[i]))]\n",
    "\n",
    "    return numpy.asarray(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign a label to a patch v\n",
    "def value_to_class(v):\n",
    "    foreground_threshold = 0.25 # percentage of pixels > 1 required to assign a foreground label to a patch\n",
    "    df = numpy.sum(v)\n",
    "    if df > foreground_threshold:\n",
    "        return [0, 1]\n",
    "    else:\n",
    "        return [1, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract label images\n",
    "def extract_labels(filename, num_images):\n",
    "    \"\"\"Extract the labels into a 1-hot matrix [image index, label index].\"\"\"\n",
    "    gt_imgs = []\n",
    "    for i in range(1, num_images+1):\n",
    "        imageid = \"satImage_%.3d\" % i\n",
    "        image_filename = filename + imageid + \".png\"\n",
    "        if os.path.isfile(image_filename):\n",
    "            print ('Loading ' + image_filename)\n",
    "            img = mpimg.imread(image_filename)\n",
    "            gt_imgs.append(img)\n",
    "        else:\n",
    "            print ('File ' + image_filename + ' does not exist')\n",
    "\n",
    "    num_images = len(gt_imgs)\n",
    "    gt_patches = [img_crop(gt_imgs[i], IMG_PATCH_SIZE, IMG_PATCH_SIZE) for i in range(num_images)]\n",
    "    data = numpy.asarray([gt_patches[i][j] for i in range(len(gt_patches)) for j in range(len(gt_patches[i]))])\n",
    "    labels = numpy.asarray([value_to_class(numpy.mean(data[i])) for i in range(len(data))])\n",
    "\n",
    "    # Convert to dense 1-hot representation.\n",
    "    return labels.astype(numpy.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ../data/training/images/satImage_001.png\n",
      "Loading ../data/training/images/satImage_002.png\n",
      "Loading ../data/training/images/satImage_003.png\n",
      "Loading ../data/training/images/satImage_004.png\n",
      "Loading ../data/training/images/satImage_005.png\n",
      "Loading ../data/training/images/satImage_006.png\n",
      "Loading ../data/training/images/satImage_007.png\n",
      "Loading ../data/training/images/satImage_008.png\n",
      "Loading ../data/training/images/satImage_009.png\n",
      "Loading ../data/training/images/satImage_010.png\n",
      "Loading ../data/training/images/satImage_011.png\n",
      "Loading ../data/training/images/satImage_012.png\n",
      "Loading ../data/training/images/satImage_013.png\n",
      "Loading ../data/training/images/satImage_014.png\n",
      "Loading ../data/training/images/satImage_015.png\n",
      "Loading ../data/training/images/satImage_016.png\n",
      "Loading ../data/training/images/satImage_017.png\n",
      "Loading ../data/training/images/satImage_018.png\n",
      "Loading ../data/training/images/satImage_019.png\n",
      "Loading ../data/training/images/satImage_020.png\n",
      "Loading ../data/training/images/satImage_021.png\n",
      "Loading ../data/training/images/satImage_022.png\n",
      "Loading ../data/training/images/satImage_023.png\n",
      "Loading ../data/training/images/satImage_024.png\n",
      "Loading ../data/training/images/satImage_025.png\n",
      "Loading ../data/training/images/satImage_026.png\n",
      "Loading ../data/training/images/satImage_027.png\n",
      "Loading ../data/training/images/satImage_028.png\n",
      "Loading ../data/training/images/satImage_029.png\n",
      "Loading ../data/training/images/satImage_030.png\n",
      "Loading ../data/training/images/satImage_031.png\n",
      "Loading ../data/training/images/satImage_032.png\n",
      "Loading ../data/training/images/satImage_033.png\n",
      "Loading ../data/training/images/satImage_034.png\n",
      "Loading ../data/training/images/satImage_035.png\n",
      "Loading ../data/training/images/satImage_036.png\n",
      "Loading ../data/training/images/satImage_037.png\n",
      "Loading ../data/training/images/satImage_038.png\n",
      "Loading ../data/training/images/satImage_039.png\n",
      "Loading ../data/training/images/satImage_040.png\n",
      "Loading ../data/training/images/satImage_041.png\n",
      "Loading ../data/training/images/satImage_042.png\n",
      "Loading ../data/training/images/satImage_043.png\n",
      "Loading ../data/training/images/satImage_044.png\n",
      "Loading ../data/training/images/satImage_045.png\n",
      "Loading ../data/training/images/satImage_046.png\n",
      "Loading ../data/training/images/satImage_047.png\n",
      "Loading ../data/training/images/satImage_048.png\n",
      "Loading ../data/training/images/satImage_049.png\n",
      "Loading ../data/training/images/satImage_050.png\n",
      "Loading ../data/training/images/satImage_051.png\n",
      "Loading ../data/training/images/satImage_052.png\n",
      "Loading ../data/training/images/satImage_053.png\n",
      "Loading ../data/training/images/satImage_054.png\n",
      "Loading ../data/training/images/satImage_055.png\n",
      "Loading ../data/training/images/satImage_056.png\n",
      "Loading ../data/training/images/satImage_057.png\n",
      "Loading ../data/training/images/satImage_058.png\n",
      "Loading ../data/training/images/satImage_059.png\n",
      "Loading ../data/training/images/satImage_060.png\n",
      "Loading ../data/training/images/satImage_061.png\n",
      "Loading ../data/training/images/satImage_062.png\n",
      "Loading ../data/training/images/satImage_063.png\n",
      "Loading ../data/training/images/satImage_064.png\n",
      "Loading ../data/training/images/satImage_065.png\n",
      "Loading ../data/training/images/satImage_066.png\n",
      "Loading ../data/training/images/satImage_067.png\n",
      "Loading ../data/training/images/satImage_068.png\n",
      "Loading ../data/training/images/satImage_069.png\n",
      "Loading ../data/training/images/satImage_070.png\n",
      "Loading ../data/training/images/satImage_071.png\n",
      "Loading ../data/training/images/satImage_072.png\n",
      "Loading ../data/training/images/satImage_073.png\n",
      "Loading ../data/training/images/satImage_074.png\n",
      "Loading ../data/training/images/satImage_075.png\n",
      "Loading ../data/training/images/satImage_076.png\n",
      "Loading ../data/training/images/satImage_077.png\n",
      "Loading ../data/training/images/satImage_078.png\n",
      "Loading ../data/training/images/satImage_079.png\n",
      "Loading ../data/training/images/satImage_080.png\n",
      "Loading ../data/training/images/satImage_081.png\n",
      "Loading ../data/training/images/satImage_082.png\n",
      "Loading ../data/training/images/satImage_083.png\n",
      "Loading ../data/training/images/satImage_084.png\n",
      "Loading ../data/training/images/satImage_085.png\n",
      "Loading ../data/training/images/satImage_086.png\n",
      "Loading ../data/training/images/satImage_087.png\n",
      "Loading ../data/training/images/satImage_088.png\n",
      "Loading ../data/training/images/satImage_089.png\n",
      "Loading ../data/training/images/satImage_090.png\n",
      "Loading ../data/training/images/satImage_091.png\n",
      "Loading ../data/training/images/satImage_092.png\n",
      "Loading ../data/training/images/satImage_093.png\n",
      "Loading ../data/training/images/satImage_094.png\n",
      "Loading ../data/training/images/satImage_095.png\n",
      "Loading ../data/training/images/satImage_096.png\n",
      "Loading ../data/training/images/satImage_097.png\n",
      "Loading ../data/training/images/satImage_098.png\n",
      "Loading ../data/training/images/satImage_099.png\n",
      "Loading ../data/training/images/satImage_100.png\n",
      "Loading ../data/training/groundtruth/satImage_001.png\n",
      "Loading ../data/training/groundtruth/satImage_002.png\n",
      "Loading ../data/training/groundtruth/satImage_003.png\n",
      "Loading ../data/training/groundtruth/satImage_004.png\n",
      "Loading ../data/training/groundtruth/satImage_005.png\n",
      "Loading ../data/training/groundtruth/satImage_006.png\n",
      "Loading ../data/training/groundtruth/satImage_007.png\n",
      "Loading ../data/training/groundtruth/satImage_008.png\n",
      "Loading ../data/training/groundtruth/satImage_009.png\n",
      "Loading ../data/training/groundtruth/satImage_010.png\n",
      "Loading ../data/training/groundtruth/satImage_011.png\n",
      "Loading ../data/training/groundtruth/satImage_012.png\n",
      "Loading ../data/training/groundtruth/satImage_013.png\n",
      "Loading ../data/training/groundtruth/satImage_014.png\n",
      "Loading ../data/training/groundtruth/satImage_015.png\n",
      "Loading ../data/training/groundtruth/satImage_016.png\n",
      "Loading ../data/training/groundtruth/satImage_017.png\n",
      "Loading ../data/training/groundtruth/satImage_018.png\n",
      "Loading ../data/training/groundtruth/satImage_019.png\n",
      "Loading ../data/training/groundtruth/satImage_020.png\n",
      "Loading ../data/training/groundtruth/satImage_021.png\n",
      "Loading ../data/training/groundtruth/satImage_022.png\n",
      "Loading ../data/training/groundtruth/satImage_023.png\n",
      "Loading ../data/training/groundtruth/satImage_024.png\n",
      "Loading ../data/training/groundtruth/satImage_025.png\n",
      "Loading ../data/training/groundtruth/satImage_026.png\n",
      "Loading ../data/training/groundtruth/satImage_027.png\n",
      "Loading ../data/training/groundtruth/satImage_028.png\n",
      "Loading ../data/training/groundtruth/satImage_029.png\n",
      "Loading ../data/training/groundtruth/satImage_030.png\n",
      "Loading ../data/training/groundtruth/satImage_031.png\n",
      "Loading ../data/training/groundtruth/satImage_032.png\n",
      "Loading ../data/training/groundtruth/satImage_033.png\n",
      "Loading ../data/training/groundtruth/satImage_034.png\n",
      "Loading ../data/training/groundtruth/satImage_035.png\n",
      "Loading ../data/training/groundtruth/satImage_036.png\n",
      "Loading ../data/training/groundtruth/satImage_037.png\n",
      "Loading ../data/training/groundtruth/satImage_038.png\n",
      "Loading ../data/training/groundtruth/satImage_039.png\n",
      "Loading ../data/training/groundtruth/satImage_040.png\n",
      "Loading ../data/training/groundtruth/satImage_041.png\n",
      "Loading ../data/training/groundtruth/satImage_042.png\n",
      "Loading ../data/training/groundtruth/satImage_043.png\n",
      "Loading ../data/training/groundtruth/satImage_044.png\n",
      "Loading ../data/training/groundtruth/satImage_045.png\n",
      "Loading ../data/training/groundtruth/satImage_046.png\n",
      "Loading ../data/training/groundtruth/satImage_047.png\n",
      "Loading ../data/training/groundtruth/satImage_048.png\n",
      "Loading ../data/training/groundtruth/satImage_049.png\n",
      "Loading ../data/training/groundtruth/satImage_050.png\n",
      "Loading ../data/training/groundtruth/satImage_051.png\n",
      "Loading ../data/training/groundtruth/satImage_052.png\n",
      "Loading ../data/training/groundtruth/satImage_053.png\n",
      "Loading ../data/training/groundtruth/satImage_054.png\n",
      "Loading ../data/training/groundtruth/satImage_055.png\n",
      "Loading ../data/training/groundtruth/satImage_056.png\n",
      "Loading ../data/training/groundtruth/satImage_057.png\n",
      "Loading ../data/training/groundtruth/satImage_058.png\n",
      "Loading ../data/training/groundtruth/satImage_059.png\n",
      "Loading ../data/training/groundtruth/satImage_060.png\n",
      "Loading ../data/training/groundtruth/satImage_061.png\n",
      "Loading ../data/training/groundtruth/satImage_062.png\n",
      "Loading ../data/training/groundtruth/satImage_063.png\n",
      "Loading ../data/training/groundtruth/satImage_064.png\n",
      "Loading ../data/training/groundtruth/satImage_065.png\n",
      "Loading ../data/training/groundtruth/satImage_066.png\n",
      "Loading ../data/training/groundtruth/satImage_067.png\n",
      "Loading ../data/training/groundtruth/satImage_068.png\n",
      "Loading ../data/training/groundtruth/satImage_069.png\n",
      "Loading ../data/training/groundtruth/satImage_070.png\n",
      "Loading ../data/training/groundtruth/satImage_071.png\n",
      "Loading ../data/training/groundtruth/satImage_072.png\n",
      "Loading ../data/training/groundtruth/satImage_073.png\n",
      "Loading ../data/training/groundtruth/satImage_074.png\n",
      "Loading ../data/training/groundtruth/satImage_075.png\n",
      "Loading ../data/training/groundtruth/satImage_076.png\n",
      "Loading ../data/training/groundtruth/satImage_077.png\n",
      "Loading ../data/training/groundtruth/satImage_078.png\n",
      "Loading ../data/training/groundtruth/satImage_079.png\n",
      "Loading ../data/training/groundtruth/satImage_080.png\n",
      "Loading ../data/training/groundtruth/satImage_081.png\n",
      "Loading ../data/training/groundtruth/satImage_082.png\n",
      "Loading ../data/training/groundtruth/satImage_083.png\n",
      "Loading ../data/training/groundtruth/satImage_084.png\n",
      "Loading ../data/training/groundtruth/satImage_085.png\n",
      "Loading ../data/training/groundtruth/satImage_086.png\n",
      "Loading ../data/training/groundtruth/satImage_087.png\n",
      "Loading ../data/training/groundtruth/satImage_088.png\n",
      "Loading ../data/training/groundtruth/satImage_089.png\n",
      "Loading ../data/training/groundtruth/satImage_090.png\n",
      "Loading ../data/training/groundtruth/satImage_091.png\n",
      "Loading ../data/training/groundtruth/satImage_092.png\n",
      "Loading ../data/training/groundtruth/satImage_093.png\n",
      "Loading ../data/training/groundtruth/satImage_094.png\n",
      "Loading ../data/training/groundtruth/satImage_095.png\n",
      "Loading ../data/training/groundtruth/satImage_096.png\n",
      "Loading ../data/training/groundtruth/satImage_097.png\n",
      "Loading ../data/training/groundtruth/satImage_098.png\n",
      "Loading ../data/training/groundtruth/satImage_099.png\n",
      "Loading ../data/training/groundtruth/satImage_100.png\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data points per class: c0 = 46309 c1 = 16191\n",
      "Balancing training data...\n",
      "32382\n",
      "(62500, 16, 16, 3)\n",
      "Number of data points per class: c0 = 16191 c1 = 16191\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"../data/training/\"\n",
    "train_data_filename = data_dir + 'images/'\n",
    "train_labels_filename = data_dir + 'groundtruth/' \n",
    "\n",
    "# Extract it into numpy arrays.\n",
    "train_data = extract_data(train_data_filename, TRAINING_SIZE)\n",
    "train_labels = extract_labels(train_labels_filename, TRAINING_SIZE)\n",
    "\n",
    "num_epochs = NUM_EPOCHS\n",
    "\n",
    "c0 = 0\n",
    "c1 = 0\n",
    "for i in range(len(train_labels)):\n",
    "    if train_labels[i][0] == 1:\n",
    "        c0 = c0 + 1\n",
    "    else:\n",
    "        c1 = c1 + 1\n",
    "print ('Number of data points per class: c0 = ' + str(c0) + ' c1 = ' + str(c1))\n",
    "\n",
    "print ('Balancing training data...')\n",
    "min_c = min(c0, c1)\n",
    "idx0 = [i for i, j in enumerate(train_labels) if j[0] == 1]\n",
    "idx1 = [i for i, j in enumerate(train_labels) if j[1] == 1]\n",
    "new_indices = idx0[0:min_c] + idx1[0:min_c]\n",
    "print (len(new_indices))\n",
    "print (train_data.shape)\n",
    "train_data = train_data[new_indices,:,:,:]\n",
    "train_labels = train_labels[new_indices]\n",
    "\n",
    "\n",
    "train_size = train_labels.shape[0]\n",
    "\n",
    "c0 = 0\n",
    "c1 = 0\n",
    "for i in range(len(train_labels)):\n",
    "    if train_labels[i][0] == 1:\n",
    "        c0 = c0 + 1\n",
    "    else:\n",
    "        c1 = c1 + 1\n",
    "print ('Number of data points per class: c0 = ' + str(c0) + ' c1 = ' + str(c1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the CNN steps\n",
    "\n",
    "1) Define the functions as conv2d, pooling, activation, flattening, dense (for the full connected steps), batch for train\n",
    "\n",
    "2) Define a CNN class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_batches(idxsize, batchsize):\n",
    "    training_indices = range(idxsize)\n",
    "    perm_indices = numpy.random.permutation(training_indices)\n",
    "\n",
    "    res = []\n",
    "    for i in range(0, idxsize, batchsize):\n",
    "        res.append(perm_indices[i:i+batchsize])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d(layer, filters_size, kernel_size, s, padding, activation='relu'):\n",
    "    if activation=='relu':\n",
    "        activation=tf.nn.relu\n",
    "    return tf.layers.conv2d(layer, filters=filters_size,  kernel_size=[kernel_size, kernel_size], strides=[s, s], padding=padding, activation=activation)\n",
    "\n",
    "def pooling(layer, k=2, s=2, pool_type='max'):\n",
    "    \n",
    "    if pool_type=='max':\n",
    "        return tf.layers.max_pooling2d(layer, pool_size=[k,k], strides=s)\n",
    "    \n",
    "def dense(layer, inputs_size, outputs_size, he_std=0.1):\n",
    "    weights = tf.Variable(tf.truncated_normal([inputs_size, outputs_size],stddev=he_std))\n",
    "    biases = tf.Variable(tf.constant(he_std, shape=[outputs_size]))\n",
    "    layer = tf.matmul(layer,weights) + biases  \n",
    "    return layer\n",
    "\n",
    "def flattening_layer(layer):\n",
    "        #make it single dimensional\n",
    "        input_size = layer.get_shape().as_list()\n",
    "        new_size = input_size[-1] * input_size[-2] * input_size[-3]\n",
    "        return tf.reshape(layer, [-1, new_size]),new_size\n",
    "    \n",
    "\n",
    "def activation(layer, activation='relu'):\n",
    "    if activation=='relu':\n",
    "        return tf.nn.relu(layer)\n",
    "    elif activation=='soft_max':\n",
    "        return tf.nn.softmax(layer)\n",
    "    \n",
    "def optimizer_choice(name='GD', lr=0.003):\n",
    "    if name=='GD':\n",
    "        return tf.train.GradientDescentOptimizer(lr)\n",
    "    elif name=='adam':\n",
    "        return tf.train.AdamOptimizer(lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN:\n",
    "    def __init__(self):\n",
    "        print('init done')\n",
    "        self.X = None\n",
    "        self.y = None\n",
    "        self.network = None\n",
    "        self.training_op = None\n",
    "        self.out = None\n",
    "        self.session = None\n",
    "    def model(self, conv_params, fc_params, optimizer='GD', learning_rate=0.003):\n",
    "\n",
    "        # init the graph \n",
    "        graph = tf.Graph()\n",
    "        graph.seed = 1\n",
    "\n",
    "\n",
    "        with graph.as_default():\n",
    "            \n",
    "            # init the placeholder (None is given for more flexibility in batch_size)\n",
    "            # 16,16 will be put as parameters in a further version\n",
    "            self.X = tf.placeholder(\n",
    "                    tf.float32,\n",
    "                    shape=[None, 16, 16, 3], name='X')\n",
    "\n",
    "            self.y = tf.placeholder(tf.float32, shape=[None, 2], name='y')\n",
    "\n",
    "\n",
    "            he_init = tf.contrib.layers.variance_scaling_initializer()\n",
    "\n",
    "            # Initialize first convolution step \n",
    "            network = conv2d(self.X, \n",
    "                             conv_params['params1']['filter_size'], \n",
    "                             conv_params['params1']['kernel_size'], \n",
    "                             conv_params['params1']['strides'],\n",
    "                             conv_params['params1']['padding'], \n",
    "                             conv_params['params1']['activation'])\n",
    "\n",
    "            network = pooling(network)\n",
    "\n",
    "            # for loop to allow different sizes of convolotional steps\n",
    "            for i in range(2,len(conv_params)+1, 1):\n",
    "                conv_par = conv_params['params'+str(i)]\n",
    "                print(conv_par)\n",
    "                network = conv2d(network, \n",
    "                             conv_par['filter_size'], \n",
    "                             conv_par['kernel_size'], \n",
    "                             conv_par['strides'],\n",
    "                             conv_par['padding'], \n",
    "                             conv_par['activation'])\n",
    "\n",
    "                network = pooling(network)\n",
    "\n",
    "            # flatten last convolution step for full connected NN\n",
    "            network, flatten_size = flattening_layer(network)\n",
    "\n",
    "            \n",
    "            # Initialize first full connected step\n",
    "            network = dense(network, \n",
    "                         flatten_size, \n",
    "                         fc_params['params1']['output_size'])\n",
    "\n",
    "            network = activation(network)\n",
    "\n",
    "            # for loop to allow different sizes of full connected NN\n",
    "            for i in range(2,len(fc_params)+1, 1):\n",
    "                fc_par = fc_params['params'+str(i)]\n",
    "                print(fc_par)\n",
    "                network = dense(network, \n",
    "                             fc_par['input_size'], \n",
    "                             fc_par['output_size'])\n",
    "\n",
    "                network = activation(network, fc_par['activation'])\n",
    "            \n",
    "            # Outputs, probability if last activation is a softmax\n",
    "            self.out = network\n",
    "\n",
    "\n",
    "            # Init the loss function (in a further version we will allow \n",
    "            # different losses (find the best to minimize F1-Score)\n",
    "            loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "            logits=self.out, labels=self.y))\n",
    "\n",
    "            ## Initialize the choosen Optimizer\n",
    "            optimizer = optimizer_choice('GD', learning_rate)\n",
    "            self.training_op = optimizer.minimize(loss)\n",
    "            \n",
    "            # Initialize all Variables\n",
    "            init = tf.global_variables_initializer()\n",
    "            \n",
    "        self.session = tf.Session(config=None, graph=graph)\n",
    "        self.session.run(init)\n",
    "\n",
    "    def train(self, train_data, train_labels, number_epochs=3, batchsize=16):\n",
    "        train_size=train_data.shape[0]\n",
    "\n",
    "        for iepoch in range(number_epochs):\n",
    "\n",
    "            batchset = prepare_batches(train_size, batchsize)\n",
    "\n",
    "            for batch_indices in batchset:\n",
    "                batch_data = train_data[batch_indices, :, :, :]\n",
    "                batch_labels = train_labels[batch_indices]\n",
    "\n",
    "                feed_dict = {\n",
    "                    self.X: batch_data,\n",
    "                    self.y: batch_labels,\n",
    "                   }\n",
    "                self.session.run(self.training_op, feed_dict=feed_dict)\n",
    "\n",
    "    def predict(self, test):\n",
    "        prediction = self.session.run(self.out, feed_dict={self.X: test})\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Easy example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_params = {\n",
    "    'params1': {'filter_size': 32 ,'kernel_size': 5, 'strides': 1, 'padding': 'SAME', 'activation': tf.nn.relu},\n",
    "    'params2': {'filter_size': 64 ,'kernel_size': 5, 'strides': 1, 'padding': 'SAME', 'activation': tf.nn.relu},\n",
    "}\n",
    "\n",
    "fc_params = {\n",
    "    'params1': {'input_size': 4*4*64, 'output_size': 512, 'activation': 'relu'},\n",
    "    'params2': {'input_size': 512, 'output_size': 2, 'activation': 'soft_max'},\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init done\n",
      "{'filter_size': 64, 'kernel_size': 5, 'strides': 1, 'padding': 'SAME', 'activation': <function relu at 0x12b8e1950>}\n",
      "{'input_size': 512, 'output_size': 2, 'activation': 'soft_max'}\n"
     ]
    }
   ],
   "source": [
    "cnn1 = CNN()\n",
    "cnn1.model(conv_params, fc_params)\n",
    "cnn1.train(train_data, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = cnn1.predict(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6320795503674881"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def accuracy(pred, labels):\n",
    "    idx0 = numpy.where(pred[:,0] <= 0.5)\n",
    "    idx1 = numpy.where(pred[:,0] > 0.5)\n",
    "\n",
    "    pred[idx0] = 0\n",
    "    pred[idx1] = 1\n",
    "\n",
    "    return (pred[:,0] == labels[:,0]).mean()\n",
    "\n",
    "accuracy(prediction, train_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
